## üîπ –û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è

- $ G = (V, E) $ ‚Äî –≥—Ä–∞—Ñ, $ |V| = n $.
- $ d(u) $ ‚Äî –º–∞—Å—Å–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π –≤ –≤–µ—Ä—à–∏–Ω–µ $ u \in V $.
- $ m $ ‚Äî —á–∏—Å–ª–æ —Ñ–∏—Ä–º.
- $ x_i \in \Delta^{n-1} $ ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏—Ä–º—ã $ i $: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º $ V $.  
  –¢–æ –µ—Å—Ç—å $ x_i(v) \geq 0 $, $ \sum_{v \in V} x_i(v) = 1 $.
- $ p_i \geq 0 $ ‚Äî —Ü–µ–Ω–∞ —Ñ–∏—Ä–º—ã $ i $.
- $ \text{dist}(u,v) $ ‚Äî –∫—Ä–∞—Ç—á–∞–π—à–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç $ u $ –¥–æ $ v $ –≤ –≥—Ä–∞—Ñ–µ.

---

## üîπ –°—Ç–æ–∏–º–æ—Å—Ç—å –¥–ª—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è

–ü—É—Å—Ç—å —Ñ–∏—Ä–º–∞ $ i $ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ $ x_i(v) $. –¢–æ–≥–¥–∞ **–æ–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å** –¥–ª—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è –≤ –≤–µ—Ä—à–∏–Ω–µ $ u $ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∏—Ä–º—ã $ i $:

$$
C_i(u) = p_i + \sum_{v \in V} x_i(v) \cdot \text{dist}(u,v)
$$

(–ª–∏–Ω–µ–π–Ω–∞—è –ø–æ $ x_i $).

---

## üîπ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —Ñ–∏—Ä–º—ã

–ü—É—Å—Ç—å –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å –≤ –≤–µ—Ä—à–∏–Ω–µ $ u $ –≤—ã–±–∏—Ä–∞–µ—Ç —Ñ–∏—Ä–º—É —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å **–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é**, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **softmin** (–Ω–∞–ø—Ä–∏–º–µ—Ä, logit-–º–æ–¥–µ–ª—å):

$$
q_i(u) = \frac{\exp(-\lambda C_i(u))}{\sum_{j=1}^m \exp(-\lambda C_j(u))}
$$

–≥–¥–µ $ \lambda > 0 $ ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä "—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏" –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è. –ü—Ä–∏ $ \lambda \to \infty $ –ø–æ–ª—É—á–∞–µ–º –∂—ë—Å—Ç–∫–∏–π –º–∏–Ω–∏–º—É–º.

---

## üîπ –û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–∏–±—ã–ª—å —Ñ–∏—Ä–º—ã

–¢–æ–≥–¥–∞ –æ–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–∏–±—ã–ª—å —Ñ–∏—Ä–º—ã $ i $:

$$
\pi_i(x_i, p_i; x_{-i}, p_{-i}) = p_i \cdot \sum_{u \in V} d(u) \cdot q_i(u)
$$

–≠—Ç–æ **–≥–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è** –ø–æ $ x_i $ –∏ $ p_i $, –µ—Å–ª–∏ $ \lambda $ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ.

---

## üîπ –°–≤—è–∑—å —Å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é

–ï—Å–ª–∏ $ x_i $ ‚Äî **–¥–µ–≥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ** (—Ç.–µ. $ x_i(v) = 1 $ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –≤–µ—Ä—à–∏–Ω—ã $ v $), –∏ $ \lambda \to \infty $, —Ç–æ:
- $ C_i(u) \to p_i + \text{dist}(u,v) $
- $ q_i(u) \to \mathbf{1}\{i = \arg\min_j C_j(u)\} $
- $ \pi_i \to $ –ø—Ä–∏–±—ã–ª—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, **—ç—Ç–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±–æ–±—â–∞–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –Ω–µ–π –Ω–∞ –≤–µ—Ä—à–∏–Ω–∞—Ö**.

---

## üîπ –ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

–î–ª—è –∫–∞–∂–¥–æ–π —Ñ–∏—Ä–º—ã $ i $:

$$
\max_{x_i \in \Delta^{n-1}, \; p_i \geq 0} \quad \pi_i(x_i, p_i; x_{-i}, p_{-i})
$$

–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–Ω—Ç—Ä–æ–ø–∏—é $ H(x_i) $) –∏–ª–∏ —à—Ç—Ä–∞—Ñ—ã –Ω–∞ —Ü–µ–Ω—É.

---

## üîπ –í–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è

- **Convex-concave procedures** (–µ—Å–ª–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å $ p_i $, –∑–∞–¥–∞—á–∞ –ø–æ $ x_i $ ‚Äî –≤—ã–ø—É–∫–ª–∞—è).
- **Mirror descent / projected gradient** –ø–æ $ x_i $ –Ω–∞ —Å–∏–º–ø–ª–µ–∫—Å–µ.
- **Alternating best response**: –ø–æ–æ—á–µ—Ä—ë–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ $ (x_i, p_i) $ –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.
- **Entropic regularization** –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è.


---

## üîπ –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

–ü—É—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏—Ä–º—ã $i$ ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ $x_i \in \Delta^{n-1}$ –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º –≥—Ä–∞—Ñ–∞ $V$. –¢–æ–≥–¥–∞ **—ç–Ω—Ç—Ä–æ–ø–∏—è** —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:

$$
H(x_i) = -\sum_{v \in V} x_i(v) \log x_i(v)
$$

–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è:
- –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏,
- –º–∏–Ω–∏–º–∞–ª—å–Ω–∞ –ø—Ä–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (—ç–Ω—Ç—Ä–æ–ø–∏—è = 0).

---

## üîπ –†–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–±—ã–ª–∏

–î–æ–±–∞–≤–∏–º —à—Ç—Ä–∞—Ñ –∑–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º $\beta > 0$. –¢–æ–≥–¥–∞ **—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–∏–±—ã–ª—å —Ñ–∏—Ä–º—ã $i$**:

$$
\tilde{\pi}_i(x_i, p_i; x_{-i}, p_{-i}) = p_i \cdot \sum_{u \in V} d(u) \cdot q_i(u) - \beta H(x_i)
$$

–≥–¥–µ:
- $q_i(u)$ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å –≤ –≤–µ—Ä—à–∏–Ω–µ $u$ –≤—ã–±–µ—Ä–µ—Ç —Ñ–∏—Ä–º—É $i$, –Ω–∞–ø—Ä–∏–º–µ—Ä:
  $$
  q_i(u) = \frac{\exp(-\lambda C_i(u))}{\sum_{j=1}^m \exp(-\lambda C_j(u))}
  $$
- $C_i(u) = p_i + \sum_{v \in V} x_i(v) \cdot \text{dist}(u,v)$

---

## üîπ –ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

–î–ª—è –∫–∞–∂–¥–æ–π —Ñ–∏—Ä–º—ã $i$:

$$
\max_{x_i \in \Delta^{n-1}, \; p_i \geq 0} \quad \tilde{\pi}_i(x_i, p_i; x_{-i}, p_{-i})
$$

–≥–¥–µ:
- $x_i$ ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–∞ —Å–∏–º–ø–ª–µ–∫—Å–µ,
- $p_i$ ‚Äî —Ü–µ–Ω–∞,
- –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ $x_{-i}, p_{-i}$ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã.

---

## üîπ –°–≤–æ–π—Å—Ç–≤–∞

- –ü—Ä–∏ $\beta = 0$: –º–æ–¥–µ–ª—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é.
- –ü—Ä–∏ $\beta \to \infty$: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π (—ç–Ω—Ç—Ä–æ–ø–∏—è ‚Üí 0).
- –ü—Ä–∏ $\lambda \to \infty$: –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç —Å—Ç—Ä–æ–≥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å ‚Üí –º–æ–¥–µ–ª—å –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π.

---

## üîπ –í–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è

- **Projected gradient ascent** –ø–æ $x_i$ –Ω–∞ —Å–∏–º–ø–ª–µ–∫—Å–µ + –ø–æ $p_i$ –Ω–∞ $\mathbb{R}_+$.
- **Mirror descent** —Å KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–µ–π ‚Äî –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π.
- **Alternating best response dynamics**: –ø–æ–æ—á–µ—Ä—ë–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ñ–∏—Ä–º.
- **JAX / PyTorch**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ, –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è.

# RL –ø–æ–¥—Ö–æ–¥ 


–ú–æ–¥–µ–ª—å –•–æ—Ç—Ç–µ–ª–∏–Ω–≥–∞ –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –∑–∞–¥–∞—á—É **–º–Ω–æ–≥o–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Multi-Agent RL)**. –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä—ë–º—Å—è, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ –∫–∞–∫–∏–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–∞—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞.

---

## üîπ –ë–∞–∑–æ–≤–∞—è RL-–ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞

### –ê–≥–µ–Ω—Ç:
–ö–∞–∂–¥–∞—è —Ñ–∏—Ä–º–∞ \(i\) ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç.

### –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π:
- **–î–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ**: –≤—ã–±–æ—Ä –≤–µ—Ä—à–∏–Ω—ã \(v_i \in V\) (–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ).
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ**: –≤–æ–∑–º–æ–∂–Ω–æ, —Ü–µ–Ω–∞ \(p_i \in \mathbb{R}_+\).

### –ü–æ–ª–∏—Ç–∏–∫–∞:
- \(\pi_i(a_i \mid s)\), –≥–¥–µ \(a_i = (v_i, p_i)\) ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏—Ä–º—ã.
- –ú–æ–∂–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞—Ç—å –∫–∞–∫ softmax over logits –Ω–∞ –≤–µ—Ä—à–∏–Ω–∞—Ö + —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ —Ü–µ–Ω–µ.

### –°–æ—Å—Ç–æ—è–Ω–∏–µ:
- –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–≥—Ä–∞), –∏–ª–∏ –≤–∫–ª—é—á–∞—Ç—å:
  - —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤,
  - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π,
  - –∏—Å—Ç–æ—Ä–∏—é —Å–ø—Ä–æ—Å–∞.

### –ù–∞–≥—Ä–∞–¥–∞:
- –ü—Ä–∏–±—ã–ª—å —Ñ–∏—Ä–º—ã:
  \[
  r_i = p_i \cdot \sum_{u \in V} d(u) \cdot \mathbf{1}\left\{i = \arg\min_j (p_j + \text{dist}(u,v_j))\right\}
  \]

---

## üîπ RL-–∞–ª–≥–æ—Ä–∏—Ç–º—ã

### 1. **Independent Q-learning / Policy Gradient**
- –ö–∞–∂–¥–∞—è —Ñ–∏—Ä–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ.
- –ü–æ–¥—Ö–æ–¥–∏—Ç –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö.
- –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ—Ç –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥o–∞–≥–µ–Ω—Ç–Ω–æ–π —Å—Ä–µ–¥–µ.

### 2. **Multi-Agent Actor-Critic (e.g., MADDPG)**
- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫—Ä–∏—Ç–∏–∫, –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏.
- –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.

### 3. **Population-Based Training / Evolutionary RL**
- –≠–≤–æ–ª—é—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤ –ø–æ–ø—É–ª—è—Ü–∏–∏ —Ñ–∏—Ä–º.
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö.

---

## üîπ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞: –ú–∞—Ä–∫–æ–≤—Å–∫–∞—è –∏–≥—Ä–∞

–ú–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∫ **–æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—É—é –∏–≥—Ä—É**, –Ω–æ —Ç–∞–∫–∂–µ –∫–∞–∫ **–ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –∏–≥—Ä—É**, –≥–¥–µ —Ñ–∏—Ä–º—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏:
- –°–æ—Å—Ç–æ—è–Ω–∏–µ: —Ç–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏ –∏ —Ü–µ–Ω—ã.
- –î–µ–π—Å—Ç–≤–∏–µ: –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è.
- –ù–∞–≥—Ä–∞–¥–∞: –ø—Ä–∏–±—ã–ª—å.
- –ü–µ—Ä–µ—Ö–æ–¥: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π.

–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **policy iteration**, **meta-RL**, **opponent modeling**.

---

## üîπ Value-—Ñ—É–Ω–∫—Ü–∏—è –∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å

–û—Ü–µ–Ω–∫–ø **Value-—Ñ—É–Ω–∫—Ü–∏–∏**:
- –î–ª—è —Ñ–∏—Ä–º—ã \(i\):  
  \[
  V_i(s) = \mathbb{E}_{\pi_i, \pi_{-i}} \left[ \sum_t \gamma^t r_i^{(t)} \mid s_0 = s \right]
  \]
- –í —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–≥—Ä–µ: –ø—Ä–æ—Å—Ç–æ –æ–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–∏–±—ã–ª—å.

–ï—Å–ª–∏ Value-—Ñ—É–Ω–∫—Ü–∏—è —Ç–æ—á–Ω–æ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ GNN –∏–ª–∏ —Ç–∞–±–ª–∏—á–Ω–æ), –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
- **Best-response dynamics**,
- **Fictitious play**,
- **Gradient ascent in policy space**.

---

## üîπ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: RL –∫–∞–∫ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –∫ Nash

–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RL –¥–ª—è –ø–æ–∏—Å–∫–∞ **–ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ–≥–æ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—è –ù—ç—à–∞**:
- –ö–∞–∂–¥–∞—è —Ñ–∏—Ä–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é Value-—Ñ—É–Ω–∫—Ü–∏—é.
- –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –∞–≥–µ–Ω—Ç –Ω–µ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å Value ‚Äî –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ.

--

- –∞–≥–µ–Ω—Ç—ã ‚Äî —Ñ–∏—Ä–º—ã;
- –¥–µ–π—Å—Ç–≤–∏—è ‚Äî –≤—ã–±–æ—Ä –≤–µ—Ä—à–∏–Ω—ã (–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ);
- –ø–æ–ª–∏—Ç–∏–∫–∞ ‚Äî softmax –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º;
- –Ω–∞–≥—Ä–∞–¥–∞ ‚Äî –¥–æ–ª—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π, –≤—ã–±—Ä–∞–≤—à–∏—Ö —Ñ–∏—Ä–º—É;
- –æ–±—É—á–µ–Ω–∏–µ ‚Äî REINFORCE —Å –ª–æ–≥–∏—Ç–∞–º–∏ –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º.

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RL-–∞–≥–µ–Ω—Ç–∞

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≥–µ–Ω—Ç–∞ $i$:
- $\theta_i \in \mathbb{R}^n$ ‚Äî –ª–æ–≥–∏—Ç—ã –ø–æ –≤–µ—Ä—à–∏–Ω–∞–º –≥—Ä–∞—Ñ–∞ $V$.
- –ü–æ–ª–∏—Ç–∏–∫–∞:  
  $$
  \pi_i(v) = \frac{\exp(\theta_i(v))}{\sum_{v'} \exp(\theta_i(v'))}
  $$

### –î–µ–π—Å—Ç–≤–∏–µ:
- –í—ã–±–æ—Ä –≤–µ—Ä—à–∏–Ω—ã $v_i \sim \pi_i$.

### –ù–∞–≥—Ä–∞–¥–∞:
- –ß–∏—Å–ª–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π, –≤—ã–±—Ä–∞–≤—à–∏—Ö —Ñ–∏—Ä–º—É $i$, –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ:
- Policy gradient (REINFORCE):  
  $$
  \theta_i \leftarrow \theta_i + \eta \cdot \nabla_{\theta_i} \log \pi_i(v_i) \cdot r_i
  $$

---